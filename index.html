<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Lovisa HagstrÃ¶m</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" style="background-color: #cec69f;" id="mainNav">
            <div class="container px-4">
                <a class="navbar-brand" href="#about">
                    <img src="figs/house-fill.svg" width="30" height="30" alt="">
                </a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li>
                        <li class="nav-item"><a class="nav-link" href="#teaching">Teaching</a></li>
                        <li class="nav-item"><a class="nav-link" href="#dissertation">Dissertation</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- About section-->
        <section id="about">
            <div class="container px-4">
                <div class="row gx-4 mt-3 mx-3 justify-content-center">
                    <div class="col-lg-4 mb-3">
                        <h3>
                            <a href="https://github.com/lovhag" target="_blank" class="link-secondary text-decoration-none">
                                <i class="bi bi-github"></i>
                            </a>
                            <a href="https://www.linkedin.com/in/lovisa-hagstrom/" target="_blank" class="link-secondary text-decoration-none">
                                <i class="bi bi-linkedin"></i>
                            </a>
                            <a href="https://scholar.google.com/citations?hl=en&user=iRYBEYsAAAAJ" target="_blank" class="link-secondary text-decoration-none">
                                <i class="ai ai-google-scholar"></i>
                            </a>
                            <a href="mailto:lovhag@chalmers.se" class="link-secondary text-decoration-none">
                                <i class="bi bi-envelope-fill"></i>
                            </a>
                        </h3>
                        <img src="figs/lovisa.jpg" class="rounded img-fluid" style="max-height: 350px" alt=""/>
                    </div>
                    <div class="col-lg-4">
                        <h3>Lovisa HagstrÃ¶m 
                            <small class="text-muted">PhD student at Chalmers University of Technology</small>
                        </h3>
                        <p class="lead">I perform research on Natural Language Processing under the supervision of <a href="https://www.cse.chalmers.se/~richajo/" target="_blank" class="text-decoration-none">Richard Johansson</a> at the <a href="https://www.chalmers.se/en/departments/cse/our-research/data-science-and-ai/" target="_blank" class="text-decoration-none">Data Science and AI division</a> at Chalmers. I'm interested in language model controllability, interpretability and prediction provenance. In my research I investigate factual knowledge in language models and methods for adding knowledge to a model.</p>
                        <br/>
                        <em>I'm happy to discuss my research and NLP in general so do reach out if you have anything on your mind!</em>
                    </div>
                </div>
            </div>
        </section>
        <!-- Publications section-->
        <section class="bg-light" id="publications">
            <div class="container px-4">
                <div class="row gx-4 mx-3 justify-content-center">
                    <div class="col-lg-8 mx-auto">
                        <h2>Publications</h2>
                        <h3>2025</h3>
                        <p>
                            <a href="https://arxiv.org/abs/2505.16518" target="_blank">CUB: Benchmarking Context Utilisation Techniques for Language Models</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong>, Youna Kim, Haeun Yu, Sang-goo Lee, Richard Johansson, Hyunsoo Cho, Isabelle Augenstein<br/>
                            <em>Arxiv</em>
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/2412.17031" target="_blank">A Reality Check on Context Utilisation for Retrieval-Augmented Generation</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong>, Sara Vera MarjanoviÄ‡, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, Isabelle Augenstein<br/>
                            <em>Accepted to ACL Main</em>
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/2410.14405" target="_blank">Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion</a><br/>
                            Denitsa Saynova, <strong>Lovisa HagstrÃ¶m</strong>, Moa Johansson, Richard Johansson, Marco Kuhlmann<br/>
                            <em>Accepted to ACL Findings</em>
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/2502.17036" target="_blank">Language Model Re-rankers are Steered by Lexical Similarities</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong>, Ercong Nie, Ruben Halifa, Helmut Schmid, Richard Johansson, Alexander Junge<br/>
                            <em>Arxiv</em>
                        </p>
                        <h3>2023</h3>
                        <p>
                            <a href="https://aclanthology.org/2023.emnlp-main.332/" target="_blank">The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong>, Denitsa Saynova, Tobias Norlund, Moa Johansson and Richard Johansson<br/>
                            <em>EMNLP</em>
                        </p>
                        <p>
                            <a href="https://research.chalmers.se/en/publication/534069" target="_blank">A Picture is Worth a Thousand Words: Natural Language Processing in Context</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong><br/>
                            <em>Licentiate thesis at Chalmers University of Technology</em>
                        </p>
                        <h3>2022</h3>
                        <p>
                            <a href="https://aclanthology.org/2022.coling-1.494" target="_blank">How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong> and Richard Johansson<br/>
                            <em>COLING</em>
                        </p>
                        <p>
                            <a href="https://aclanthology.org/2022.acl-srw.19" target="_blank">What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong> and Richard Johansson<br/>
                            <em>ACL Student Research Workshop</em>
                        </p>
                        <p>
                            <a href="https://aclanthology.org/2022.clasp-1.5" target="_blank">Can We Use Small Models to Investigate Multimodal Fusion Methods?</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong>, Tobias Norlund and Richard Johansson<br/>
                            <em>CLASP Conference on (Dis)embodiment</em>
                        </p>
                        <h3>2021</h3>
                        <p>
                            <a href="https://aclanthology.org/2021.blackboxnlp-1.10" target="_blank">Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?</a><br/>
                            Tobias Norlund, <strong>Lovisa HagstrÃ¶m</strong> and Richard Johansson<br/>
                            <em>BlackboxNLP Workshop</em>
                        </p>
                        <p>
                            <a href="https://aclanthology.org/2021.nodalida-main.13" target="_blank">Knowledge Distillation for Swedish NER models: A Search for Performance and Efficiency</a><br/>
                            <strong>Lovisa HagstrÃ¶m</strong> and Richard Johansson<br/>
                            <em>Nordic Conference on Computational Linguistics (NoDaLiDa)</em>
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Teaching section-->
        <section id="teaching">
            <div class="container px-4">
                <div class="row gx-4 mx-3 mb-5 justify-content-center">
                    <div class="col-lg-8 mx-auto">
                        <h2>Teaching</h2>
                        <p class="lead">
                            I supervise master's theses at Chalmers and would be happy to discuss potential thesis projects with students or organizations interested in NLP. Simply send me an email! <br/><br/>
                            
                            I also work as a teaching assistant for the courses <a href="https://www.chalmers.se/en/education/your-studies/find-course-and-programme-syllabi/course-syllabus/DAT435/?acYear=2022%2F2023" target="_blank" class="text-decoration-none">Applied mathematical thinking</a>, <a href="https://www.chalmers.se/en/education/your-studies/find-course-and-programme-syllabi/course-syllabus/TDA233/?acYear=2022/2023" target="_blank" class="text-decoration-none">Algorithms for machine learning and inference</a> and <a href="https://www.chalmers.se/en/education/your-studies/find-course-and-programme-syllabi/course-syllabus/DAT341/?acYear=2022%2F2023" target="_blank" class="text-decoration-none">Applied Machine Learning</a> at Chalmers. 
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Dissertation section-->
        <section class="bg-light" id="dissertation">
            <div class="container px-4">
                <div class="row gx-4 mx-3 justify-content-center">
                    <div class="col-lg-8 mx-auto">
                        <h2>PhD Dissertation Defense</h2>
                        <p class="lead">
                            <strong>Title:</strong> Language Models and Knowledge Representations.<br/>
                            <strong>When:</strong> September 2025.<br/>
                            <strong>Where:</strong> Room TBA, Chalmers University of Technology, Gothenburg, Sweden.<br/>
                        </p>
                        <p class="lead">
                            My doctoral studies have mainly focused on the intersection between language models (LMs) and knowledge representations. Given that LMs are increasingly used as simpler interfaces to factual knowledge, we need models that not only are accurate, but also factually consistent, updatable and, ultimately, reliable. The body of work that will be discussed during my PhD defense touch upon these topics in different manners:
                        </p>
                        <fieldset>
                        <legend class="text-muted">ðŸ“‘ #1</legend>
                            <div class="text">
                            <h3>Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?</h3>
                            <p>
                                <a href="https://aclanthology.org/2021.blackboxnlp-1.10/" class="btn btn-secondary btn-sm" style="--bs-btn-padding-y: .1rem">BlackboxNLP Workshop 2021</a>
                            </p>
                            LM hallucinations, i.e. inaccurate and potentially misleading model outputs, may originate from training on incomplete knowledge sources. For example, text data suffers from reporting bias such that it mainly reports novel information rather than trivial information. In this paper, we hypothesise that LMs may form more complete knowledge representations if they are trained on more comprehensive sources of knowledge, such as information from additional modalities apart from text. We investigate whether LMs can obtain and represent knowledge from training on visual information, to be able to express it in a textual modality. We propose a dataset, Memory Colors, for evaluating this transfer capability and a model, CLIP-BERT, that can be trained on visual information. Our results show that it is possible to leverage visual information training to complement the knowledge of a LM.
                        </fieldset>
                        <fieldset>
                        <legend class="text-muted">ðŸ“‘ #2</legend>
                            <div class="text">
                            <h3>The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models</h3>
                            <p>
                                <a href="https://aclanthology.org/2023.emnlp-main.332/" class="btn btn-secondary btn-sm" style="--bs-btn-padding-y: .1rem">EMNLP 2023</a>
                            </p>
                            LMs struggle to provide consistent answers to paraphrased questions asking for factual information. This is problematic, as LMs should be consistent when used as interfaces to factual knowledge. In this paper, we investigate potential approaches for improving the factual consistency of LMs. We find that retrieval augmentation, as represented by the Atlas model, works best for improving model consistency compared model upscaling, for which an Atlas model with 330M parameters and access to Wikipedia outperforms a Llama model with 65B parameters. We also find that syntactic form impacts consistency â€“ LMs are more likely to fail to provide consistent predictions if the consistent prediction results in an unidiomatic sentence. 
                        </fieldset>
                        <fieldset>
                            <legend class="text-muted">ðŸ“‘ #3</legend>
                            <div class="text">
                            <h3>A Reality Check on Context Utilisation for Retrieval-Augmented Generation</h3>
                            <p>
                                <a href="https://arxiv.org/abs/2412.17031" class="btn btn-secondary btn-sm" style="--bs-btn-padding-y: .1rem">Accepted to ACL Main 2025</a>
                            </p>
                            Retrieval-augmented generation (RAG), i.e. retrieving contexts from an external knowledge source, can be used to alleviate limitations of the parametric knowledge of LMs. However, investigations of how LMs utilise retrieved information of varying complexity in real-world scenarios have been limited to synthetic contexts. In this paper, we introduce DRUID, a dataset with real-world queries and contexts. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) to find that artificial datasets often fail to represent the complex and diverse real-world context settings, yielding misleading results when used to study LM context utilisation. Overall, this paper underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings.
                        </fieldset>
                        <fieldset>
                            <legend class="text-muted">ðŸ“‘ #4</legend>
                            <div class="text">
                            <h3>Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion</h3>
                            <p>
                                <a href="https://arxiv.org/abs/2410.14405" class="btn btn-secondary btn-sm" style="--bs-btn-padding-y: .1rem">Accepted to ACL Findings 2025</a>
                            </p>
                            Moving back to studying the parametric knowledge of LMs, this paper inspects previous interpretations of LMs for fact completion from the perspective of diagnostic data. Previous model interpretations pay little attention to the samples used to elicit and study factual associations of LMs, and it is not known whether samples of different characteristics may elicit different factual associations in LMs. In this paper, we identify four different prediction scenarios for fact completion: Generic language modeling, guesswork, heuristics recall and exact fact recall. We introduce a method for creating datasets with samples of each prediction scenario and show how two popular interpretability methods (causal tracing, studies of information flow) yield distinct results for each scenario. Results for exact fact recall and generic language modeling scenarios confirm previous conclusions about the importance of mid-range MLP sublayers for fact recall, while results for guesswork and heuristics indicate a critical role of late last token position MLP sublayers. In summary, this paper contributes resources for a more extensive and granular study of fact completion in LMs, together with analyses that provide a more nuanced understanding of how LMs process fact-related queries.
                        </fieldset>
                        <fieldset>
                            <legend class="text-muted">ðŸ“‘ #5</legend>
                            <div class="text">
                            <h3>CUB: Benchmarking Context Utilisation Techniques for Language Models</h3>
                            <p>
                                <a href="https://arxiv.org/abs/2505.16518" class="btn btn-secondary btn-sm" style="--bs-btn-padding-y: .1rem">Arxiv</a> <small>(under review for EMNLP 2025)</small>
                            </p> 
                            Building on Paper #3, this paper takes a closer look at LM context utilisation and methods for manipulating context utilisation. Context utilisation is a key component of LMs used for RAG, as the benefits of retrieving external information are only realised if the generative model makes adequate use of the retrieved information. Many context utilisation manipulation techniques (CMTs) have recently been proposed, while none have seen unified comparison. We develop CUB (Context Utilisation Benchmark) to allow for a comprehensive evaluation and comparison of CMTs. CUB systematically tests the sensitivity of CMTs to underlying model and naturally occurring context types (gold, conflicting and irrelevant) on tasks representative of synthesised and realistic RAG scenarios. In summary, this paper provides a deeper analysis of what CMT works best for a given scenario and identified areas of improvement for CMTs.
                        </fieldset>
                    </div>
                </div>
            </div>
        </section>
        <!-- Footer-->
        <footer class="py-4">
            <div class="container px-4"><p class="m-0 text-center text-black">&copy; Lovisa HagstrÃ¶m 2023</p></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
